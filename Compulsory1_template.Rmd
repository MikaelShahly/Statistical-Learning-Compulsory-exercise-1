---
subtitle: "TMA4268 Statistical Learning V2024"
title: "Compulsory exercise 1: Group 31"
author: "Emil Jønsrud, Mikael Shahly, Sindre Nogva Vik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  pdf_document:
    extra_dependencies : ["amsmath"]
---

```{r,echo=FALSE}
# install.packages("knitr") # probably already installed
# install.packages("rmarkdown") # probably already installed
# install.packages("ggplot2") # plotting with ggplot2
# install.packages("dplyr") # for data cleaning and preparation
# install.packages("ggfortify") # for model checking
#install.packages("MASS")
#install.packages("tidyr")
#install.packages("carData") # dataset
#install.packages("class")
#install.packages("pROC")
#install.packages("plotROC")
#install.packages("boot")
#install.packages("ggmosaic")
library("knitr")
library("rmarkdown")
library("ggfortify")
library("MASS")
```

# Problem 1

For this problem you will need to include some LaTex code. Please install latex on your computer and then consult Compulsor1.Rmd for hints how to write formulas in LaTex

## a)

Qualitative variable: Is a categorical variable. In a cat / dog classifiers examples of a qualitiave variable is face shape (round / square), ear shape (round / pointed) and country of origin.

Quantitative variable: Is a continous variable. I the dog / cat classifier examples would be weight, tail length, amount of teeth.

## b)

LDA, QDA, KNN

## c)

$var(\epsilon)$ - is irreducible error term, this term will represent the irreducible error in our cost function (MSE). Note $\epsilon$ is the random variable that represents the inherent noise in our data.

$var(\hat{f}(X))$ - represents the variance. So how much the estimator will change when there are changes in the training data (so it represents its ability to generalize to a new unseen dataset, high varience implies that it does not generalize to unseen datasets well)

$E([(f(x) - \hat{f}(x)])^2$ - represents the bias, so the expected error between the dataset and our estimator

The derivation of the formula is:
```{=latex}
\begin{align*}
E[(y - \hat{y} )^2]
&= E[f(x) + \epsilon - \hat{f}(x)^2] \\
&= E[f(x)^2] + E[\hat{f}(x)]^2 + E[\epsilon^2] + E[\hat{f}(x)^2] + E[2f(x)\epsilon] + E[-2\epsilon \hat{f}(x)] -2E[f(x)\hat{f}(x)] \\
&= f(x)^2 + \epsilon^2 + E[\hat{f}(x)]^2 + 2f(x)E[\epsilon] - 2E[\epsilon]E[\hat{f}(x)] + -2E[f(x)\hat{f}(x)] \\
&= f(x)^2 + E[\epsilon^2] + -2f(x)E[\hat{f}(x)] + E[\hat{f}(x)]^2 + var[\hat{f}(x)]\\
&= E[(f(x) - \hat{f}(x))^2] + var[\hat{f}(x)]+ var[\epsilon]
\end{align*}
```

The main calculations are that $var[\epsilon] = E[\epsilon]^2 + E[\epsilon^2]$ where $E[\epsilon] =0$. f(x) is not a random variable and is therefore treated like a constant. $\epsilon$ is considered independent from $\hat{f}(x)$ as $\epsilon$ is uncorrelated noise.

## d)

k = 1 : blue

k = 3 : red

k = 5: red

## e)

```{r}
#import boston housing dataset
library(MASS)
data(Boston)

lm_1 <- lm(medv~rm + age, data=Boston)
summary(lm_1)
```

```{r}
cor_matrix <- cor(Boston[c("rm", "medv", "age")])
cor_matrix
```

```{r}
lm_2 = lm(medv~rm + age + nox, data=Boston)
summary(lm_2)
```

The p-value is a hypothesis test with H0 = the features independant from the target values (uncorrelated, so the corresponding slope estimator for the feature is zero $\beta = 0$) . Lets check the correlation of nox and age

```{r}
cor_nox_age <- cor(Boston[c("nox", "age")])
print(cor_nox_age)
```

As we can see nox and age are closely correlated. This means both the feature likely play a large role in creating the model. They then have a similare effect on the regression model and therefore the significance of each predictor is reduced in the model (compared to making a linear regression model with only age or only nox).

# Problem 2

## a)

```{r, eval=TRUE, echo=TRUE}
lm_3 <- lm(medv ~ crim + age + crim*age + rm + rm^2, data=Boston)
summary(lm_3)
```

When $x_{\text{crim}}$ is increased by 10 while $x_{\text{age}}$ is held constant at 60, the resulting change to medv will be given by the slope coefficients related to the predictors given by crim ( $x_{\text{crim}}$ and $x_{\text{crim} * \text{age}}$).

This implies that $\delta\hat{\text{medv}} = \delta{x_\text{crim}} * \beta_\text{crim} + \delta x_\text{crim*age} \beta_\text{crim * age}$

which gives a change of $\delta\hat{\text{medv}} = -5.29827 * 10^3$

```{r}
#some independent analysis of the dataset
cor_matrix <- cor(Boston[c("medv", "dis")])
lm_dis <- lm(medv ~ dis, data=Boston)
residuals_lm_dis <- residuals(lm_dis)
autoplot(lm_dis)
summary(lm_dis)
```

## b)

To reduce the the standard error of the slope estimators $\boldsymbol{\hat\beta}$ we can increase the amount of data collected. This is because the $\hat{SE}(\hat{\beta_i})$) is given by dividing the estimator $\hat{\sigma}$ of the standard deviation by the sum of differences from the mean squared.

By increasing the amount of data we reduce $\hat{\sigma}$ and therefore $\hat{SE}(\hat{\beta_i})$

## c)

```{r}
lm_4 <- lm(medv ~ crim + age + rm, data = Boston)
summary(lm_4)
```

$H_0: \hat{\beta_\text{rm}} = 0$ - predictor has no correlation with predicted target value $\hat{y}$

#### ii)

```{=latex}
\begin{align*}
H_{0}&: \beta_{\alpha} = 0 &&\forall \alpha \in \{ \text{crim}, \text{age}, \text{rm} \}  \\
H_{1}&: \beta_{\alpha} \neq 0 && \text{for at least one } \alpha 
\end{align*}
```

```{r}
#under H0 the F-value is fisher distributed
p <- length(lm_4$coefficients)-1 #length(coef(lm_4))
k <- length(lm_4$coefficients)-1 #length(coef(lm_4))
n <- nobs(lm_4)
TSS <- sum((Boston["medv"] - mean(Boston$medv))^2)
RSS <- sum((Boston["medv"] - predict(lm_4))^2)
SSE <- TSS - RSS

F_val = ((TSS - RSS) / p) / (RSS / (n - p - 1))
p_val = pf(F_val, k, n-p, lower.tail=FALSE) 
print(p_val)
```

#### iii)

```{r}
#for new linear regression model
lm_5 = lm(medv ~ crim + age, data = Boston)
summary(lm_5)


p <- length(lm_5$coefficients)-1 #length(coef(lm_4))
k <- length(lm_5$coefficients)-1 #length(coef(lm_4))
n <- nobs(lm_5)
TSS <- sum((Boston["medv"] - mean(Boston$medv))^2)
RSS <- sum((Boston["medv"] - predict(lm_5))^2)
SSE <- TSS - RSS

F_val = ((TSS - RSS) / p) / (RSS / (n - p - 1))
p_val = pf(F_val, k, n-p, lower.tail=FALSE) 
print(p_val)
```

## d)

#### i)

Confidence interval is:

```{r}
new_obs_df = data.frame(crim=10, age=90, rm=5)
print(predict(lm_4, new_obs_df, interval="confidence", type="respons"))
```

#### ii)

```{r}
print(predict(lm_4, new_obs_df, interval="prediction", type="respons"))
```

#### iii)

Confidence interval is the interval for where we can say with a certain amount of level of probability that the our linear regression line is (so a confidence interval for our $\beta_{i}$ estimator).

The prediction interval is the interval where we can say a new observation (so a new y observation) must be within. This is will then contain the uncertainty from our $\beta_{i}$ and our irreducible errors.

The prediction interval must always be bigger then the confidence interval as it contains more uncertainty

#### iiii)

```{r}
#Autoplot will make all the relevant plots for us
autoplot(lm_4)
```

The QQ plot describes the distribution of our residuals to a normal distribution. If our residuals are normally distributed the plot will be a straight line (as both the residuals and the normal distribution compared to are equal). We see that our Q-Q plot our residuals are approximately normally distributed close to the 0 quantile, while it increases as we move away from it. This implies that the residuals are not normally distributed (maybe t-distributed based on the Q-Q plot) as we assume when creating the linear regression estimator.

The leverage is how large effect a sample from our data set has on the linear regression estimator, the main factor here is distance from the midpoint of the linear regressor in the feature space. A sample from our data set with a large residual, but low leverage will have a small effect on the linear regression model. The flagged samples in the plot are likely outliers

The scale-location plot is used to check the assumption of equal variance between all the residuals. We then plot each standardized residual to the prediction value $\hat{y}$. Equal variances implies no trend. Based on this it seems like the assumption of equal variance in each residual is not correct in our data.

Tukey-Anscombe plot is similare to scale-location just using the residual instead of squared standardized residuals. It should be centered in 0 ( 0 mean) and not have a trend.

## e)

#### i)

This is incorrect as $x_\text{male}$ and $x_\text{female}$ is encoding the same information.

#### ii)

we must remove one of the predictors. Which one it is has no meaning $y = \beta_0 + \beta_1*x_\text{male}$ would be a valid formulation.

#### iii)

if we pick bachelor as our reference category and chose to not include it, we would get: $y = \beta_0 + \beta_1x_\text{master} + \beta_2x_\text{phd}$

## f)

i)  False
ii) False
iii) True
iv) False

# Problem 3

```{r}
set.seed(123)
# prepare the dataset into training and test datasets
library(titanic)
data("titanic_train")

# remove some variables that are difficult to handle.
# NB! after the removal, the datasets have the variable names of
# [Survived, Pclass, Sex, Age, SibSp, Parch, Fare].
vars_to_be_removed <- c("PassengerId", "Name", "Ticket", "Cabin", "Embarked")
titanic_train <- titanic_train[, -which(names(titanic_train) %in% vars_to_be_removed)]

# make Pclass a categorical variable
titanic_train$Pclass <- as.factor(titanic_train$Pclass)

# divide the dataset into training and test datasets
train_idx <- sample(1:nrow(titanic_train), 0.8 * nrow(titanic_train))
titanic_test <- titanic_train[-train_idx, ]
titanic_train <- titanic_train[train_idx, ]
```

```{r}
logReg <- glm(Survived ~ ., data=titanic_train, family=binomial)
summary(logReg)
```

```{r}
prediction_prob <- predict(logReg, new_data=titanic_test, type="response")
predictions <- as-factor(ifelse(prediction_prob > 0.5, 1, 0))
test_accuracy <- mean(titanic_test$Survived == predictions)
```

We now do a hypothesis test with the null hypothesis $H_0: \beta_\text{Pclass} = 0$ which implies that the predictor has no predictive power in the model.

```{r}
#Fit a logistic reg model without Pclass
logReg2 <- glm(Survived ~ . -Pclass, data=titanic_train, family=binomial)

anova(logReg, logReg2, test="Chisq")

```

As the p-value is very low we can reject our null hypothesis.

```{r}



new_obs_dfTit = data.frame(Pclass = factor(1), Sex = "female", Age = 40, SibSp = 1, Parch = 0, Fare = 200)
new_obs_dfTit2 = data.frame(Pclass = factor(3), Sex = "female", Age = 40, SibSp = 1, Parch = 0, Fare = 20)

print(predict(logReg, new_obs_dfTit))
print(predict(logReg, new_obs_dfTit2))
```

```{r}
#iv)

lda_fit = lda(Survived ~ ., data=titanic_train)
prediction_prob_lda <- predict(lda_fit, new_data=titanic_test, type="response") #hvorfor så lang?
predictions_lda <- as.factor(ifelse(prediction_prob_lda$posterior > 0.5, 1, 0))
test_accuracy_lda <- mean(titanic_test$Survived == predictions_lda)
print(test_accuracy_lda)
```

```{r}
qda_fit = qda(Survived ~ ., data=titanic_train)

prediction_prob_qda <- predict(qda_fit, new_data=titanic_test, type="response")
predictions_qda <- as.factor(ifelse(prediction_prob_qda$posterior > 0.5, 1, 0))
test_accuracy_qda <- mean(titanic_test$Survived == predictions_lda)
print(test_accuracy_qda)
```

```{r}
#Making ROC curve for logReg¨
#spør studass om hvorfor vi får ROC curves - plot of sensisvity ~ (1 - sensitivity)
#Note, the plot is for different threshold values (so the threshold acts as a time #variable)
library(pROC)
```

## b)

#### i)

In both paradigms we try to estimate $P(Y=k | X=x)$ but the difference is how we estimate it.

In the diagnostic paradigm we directly try to estimate $P(Y=k | X=x)$

#### ii)

diagnostic: logistic regression, KNN sampling: Naive Bayes classifier, LDA, QDA

## c)

### ii)

```{r}

set.seed(123) # Replace 123 with any number of your choice
# generate data for the two normal distributions
n_samples_class1 <- 3000
n_samples_class2 <- 7000
x1 <- rnorm(n_samples_class1, mean = -2, sd = 1.5)
x2 <- rnorm(n_samples_class2, mean = 2, sd = 1.5)
# create a data frame with the generated data
df <- data.frame(X1 = c(x1, x2), class = c(rep(1, n_samples_class1), rep(2, n_samples_class2)))
8
# fit LDA
lda_model <- lda(class ~ ., data = df)

```

### iii)

```{r}
# predict p_k(x) using the fitted LDA model
df_1 <- df[df$class == 1,]["X1"]
df_2 <- df[df$class == 2,]["X1"]

p_1_x <- predict(lda_model, df_1) # compute p_1(X)
p_2_x <- predict(lda_model, df_2) # compute p_2(X)
p_tot <- predict(lda_model) #computes both in posterior
```

### iv)

```{r}
plot(df$X1, p_tot$posterior[,1])
points(df$X1, p_tot$posterior[,2])

#plot((p_2_x$posterior[,1], df_2$X1))
```

## d)

i. False

ii. True

iii. True

iv. False

# Problem 4

## a)

Answer iv) is the correct answer

## b)

```{r}
set.seed(123)
# Import the Boston housing price dataset
library(caret)
data(Boston)

# select specific variables
selected_vars <- c("crim", "rm", "age", "medv")
boston_selected <- Boston[, selected_vars]

# manually perform the 5-fold cross-validation
folds <- createFolds(boston_selected$medv, k = 5) ## K = 5
rmse_list <- list()

for (i in 1:length(folds)) {
  # get the training and validation sets
  ##incorrect, we should use 1 fold for val and the rest for training
  train <- boston_selected[-folds[[i]], ]
  val <- boston_selected[folds[[i]], ]
  
  # fit a linear regression model
  model <- lm(medv ~ ., data = train)
  
  # compute RMSE on the validation set
  pred <- predict(model, val)
  ##INCORRECT: ERRORS NOT SQUARED
  rmse <- sqrt(mean((pred - val$medv)^2)) # root mean squared error (RSME)
  rmse <- rmse[1] # take out the value
  # store rmse in rmse_list
  rmse_list[[i]] <- rmse
}

# compute mean of rmse_list
rmse_mean <- mean(as.numeric(rmse_list))
cat("rmse_mean:", rmse_mean, "\n")

```

```{r}
set.seed(123)
# Import the Boston housing price dataset
library(caret)
data(Boston)

# select specific variables
selected_vars <- c("crim", "rm", "age", "medv")
boston_selected <- Boston[, selected_vars]

# manually perform the 5-fold cross-validation
folds <- createFolds(boston_selected$medv, k = length(Boston$medv))
rmse_list <- list()

for (i in 1:length(folds)) {
  # get the training and validation sets
  ##incorrect, we should use 1 fold for val and the rest for training
  train <- boston_selected[-folds[[i]], ]
  val <- boston_selected[folds[[i]], ]
  
  # fit a linear regression model
  model <- lm(medv ~ ., data = train)
  
  # compute RMSE on the validation set
  pred <- predict(model, val)
  ##INCORRECT: ERRORS NOT SQUARED
  rmse <- sqrt(mean((pred - val$medv)^2)) # root mean squared error (RSME)
  rmse <- rmse[1] # take out the value
  # store rmse in rmse_list
  rmse_list[[i]] <- rmse
}

# compute mean of rmse_list
rmse_mean <- mean(as.numeric(rmse_list))
cat("rmse_mean:", rmse_mean, "\n")
```

## c)

### i.

```{r}
# simulate data (no need to change this part)
set.seed(123)
n <- 1000 # population size
dataset <- rnorm(n) # population

# bootstrap
B <- 1000 # CORR: bootstrap sample size shold be larger
boot <- numeric(B) # CORR: matix is not needed
for (i in 1:B) {
  boot[i] <- median(sample(dataset, n, replace = TRUE)) # CORR
}


# compute the standard error of the median from the bootstrap samples
standard_erorr_of_the_median_bootstrap <- sd(boot)
cat("standard_erorr_of_the_median_bootstrap:", standard_erorr_of_the_median_bootstrap, "\n")

```

### ii.

```{r}
# simulate data (no need to change this part)
set.seed(123)
n <- 1000 # population size
dataset <- rnorm(n) # population

# bootstrap
B <- 1000 # CORR: bootstrap sample size shold be larger
boot_replace <- numeric(B) # CORR: matix is not needed
boot_no_replace <- numeric(B) # CORR: matix is not needed

for (i in 1:B) {
  boot_replace[i] <- median(sample(dataset, n, replace = TRUE)) # CORR
  boot_no_replace[i] <- median(sample(dataset, n, replace = FALSE)) # CORR
}


# compute the standard error of the median from the bootstrap samples
standard_erorr_of_the_median_bootstrap <- sd(boot_replace)
cat("standard_erorr_of_the_median_bootstrap:", standard_erorr_of_the_median_bootstrap, "\n")
standard_erorr_of_the_median_bootstrap <- sd(boot_no_replace)
cat("standard_erorr_of_the_median_bootstrap without replacement:", standard_erorr_of_the_median_bootstrap, "\n")
```

Using `replace=FALSE` restricts the sampling to not allow for resampling with replacement. This contradicts the idea of bootstrapping, as we are preventing any data point from being selected more that once.

## d)

i.  True
ii. False
iii. True
iv. False
