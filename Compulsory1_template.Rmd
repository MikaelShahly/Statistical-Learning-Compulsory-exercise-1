---
subtitle: "TMA4268 Statistical Learning V2024"
title: "Compulsory exercise 1: Group XYZ (add your group number here)"
author: "NN1, NN2 and NN3 (full names of all group members)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  pdf_document:
    extra_dependencies : ["amsmath"]
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE)
```

```{r,eval=TRUE,echo=FALSE}
# install.packages("knitr") # probably already installed
# install.packages("rmarkdown") # probably already installed
# install.packages("ggplot2") # plotting with ggplot2
# install.packages("dplyr") # for data cleaning and preparation
# install.packages("ggfortify") # for model checking
#install.packages("MASS")
#install.packages("tidyr")
#install.packages("carData") # dataset
#install.packages("class")
#install.packages("pROC")
#install.packages("plotROC")
#install.packages("boot")
#install.packages("ggmosaic")
library("knitr")
library("rmarkdown")
library("ggfortify")
```

<!--  Etc (load all packages needed). -->

# Problem 1

For this problem you will need to include some LaTex code. Please install latex on your computer and then consult Compulsor1.Rmd for hints how to write formulas in LaTex

## a)

Qualitative variable: Is a categorical variable. In a cat / dog classifiers examples of a qualitiave variable is face shape (round / square), ear shape (round / pointed) and country of origin.

Quantitative variable: Is a continous variable. I the dog / cat classifier examples would be weight, tail length, amount of teeth.

## b)

LDA, QDA, KNN

## c)

$var(\epsilon)$ - is irreducible error term, this term will represent the irreducible error in our cost function (MSE). Note $\epsilon$ is the random variable that represents the inherent noise in our data.

$var(\hat{f}(X))$ - represents the variance. So how much the estimator will change when there are changes in the training data (so it represents its ability to generalize to a new unseen dataset, high varience implies that it does not generalize to unseen datasets well)

$E([(f(x) - \hat{f}(x)])^2$ - represents the bias, so the expected error between the dataset and our estimator

The derivation of the formula is:$$
E[(y - \hat{y} )^2] \\
= E[f(x) + \epsilon - \hat{f}(x)^2] \\
= E[f(x)^2] + E[\hat{f}(x)]^2 + E[\epsilon^2] + E[\hat{f}(x)^2] + E[2f(x)\epsilon] + E[-2\epsilon \hat{f}(x)] -2E[f(x)\hat{f}(x)] \\
= f(x)^2 + \epsilon^2 + E[\hat{f}(x)]^2 + 2f(x)E[\epsilon] - 2E[\epsilon]E[\hat{f}(x)] + -2E[f(x)\hat{f}(x)] \\
= f(x)^2 + E[\epsilon^2] + -2f(x)E[\hat{f}(x)] + E[\hat{f}(x)]^2 + var[\hat{f}(x)]\\
= E[(f(x) - \hat{f}(x))^2] + var[\hat{f}(x)]+ var[\epsilon]
$$

The main calculations are that $var[\epsilon] = E[\epsilon]^2 + E[\epsilon^2]$ where $E[\epsilon] =0$. f(x) is not a random variable and is therefore treated like a constant. $\epsilon$ is considered independent from $\hat{f}(x)$ as $\epsilon$ is uncorrelated noise.

## d)

k = 1 : blue

k = 3 : red

k = 5: red

## e)

```{r}
#import boston housing dataset
library(MASS)
data(Boston)

lm_1 <- lm(medv~rm + age, data=Boston)
summary(lm_1)
```

```{r}
cor_matrix <- cor(Boston[c("rm", "medv", "age")])
cor_matrix
```

```{r}
lm_2 = lm(medv~rm + age + nox, data=Boston)
summary(lm_2)
```

The p-value is a hypothesis test with H0 = the features independant from the target values (uncorrelated, so the corresponding slope estimator for the feature is zero $\beta = 0$) . Lets check the correlation of nox and age

```{r}
cor_nox_age <- cor(Boston[c("nox", "age")])
print(cor_nox_age)
```

As we can see nox and age are closely correlated. This means both the feature likely play a large role in creating the model. They then have a similare effect on the regression model and therefore the significance of each predictor is reduced in the model (compared to making a linear regression model with only age or only nox).

# Problem 2

## a)

```{r, eval=TRUE, echo=TRUE}
lm_3 <- lm(medv ~ crim + age + crim*age + rm + rm^2, data=Boston)
summary(lm_3)
```

When $x_{\text{crim}}$ is increased by 10 while $x_{\text{age}}$ is held constant at 60, the resulting change to medv will be given by the slope coefficients related to the predictors given by crim ( $x_{\text{crim}}$ and $x_{\text{crim} * \text{age}}$).

This implies that $\delta\hat{\text{medv}} = \delta{x_\text{crim}} * \beta_\text{crim} + \delta x_\text{crim*age} \beta_\text{crim * age}$

which gives a change of $\delta\hat{\text{medv}} = -5.29827 * 10^3$

```{r}
#some independent analysis of the dataset
cor_matrix <- cor(Boston[c("medv", "dis")])
lm_dis <- lm(medv ~ dis, data=Boston)
residuals_lm_dis <- residuals(lm_dis)
autoplot(lm_dis)
summary(lm_dis)
```

## b)

To reduce the the standard error of the slope estimators $\boldsymbol{\hat\beta}$ we can increase the amount of data collected. This is because the $\hat{SE}(\hat{\beta_i})$) is given by dividing the estimator $\hat{\sigma}$ of the standard deviation by the sum of differences from the mean squared.

By increasing the amount of data we reduce $\hat{\sigma}$ and therefore $\hat{SE}(\hat{\beta_i})$

## c)

```{r}
lm_4 <- lm(medv ~ crim + age + rm, data = Boston)
summary(lm_4)
```

$H_0: \hat{\beta_\text{rm}} = 0$ - predictor has no correlation with predicted target value $\hat{y}$

#### ii)

\$\$ \begin{align}
H_{0}&: \beta_{\alpha} = 0 &&\forall \alpha \in \{ \text{crim}, \text{age}, \text{rm} \}  \\

H_{1}&: \beta_{\alpha} \neq 0 && \text{for at least one } \alpha 
\end{align} \$\$

```{r}
#under H0 the F-value is fisher distributed
p <- length(coef(lm_4)) + 1
k <- length(coef(lm_4))
n <- nobs(lm_4)
TSS <- sum(Boston["medv"] - mean(Boston$medv)^2)
RSS <- sum(Boston["medv"] - mean(predict(lm_4))^2)
SSE <- TSS - RSS

F = ((TSS - RSS) / p) / (RSS / (n - p - 1))
p_val = pf(F, k, n-p, lower.tail=FALSE) 
print(p_val)
```

#### iii)

```{r}
#for new linear regression model
lm_5 = lm(medv ~ crim + age, data = Boston)


p <- length(coef(lm_5)) + 1
k <- length(coef(lm_5))
n <- nobs(lm_5)
TSS <- sum(Boston["medv"] - mean(Boston$medv)^2)
RSS <- sum(Boston["medv"] - mean(predict(lm_5))^2)
SSE <- TSS - RSS

F = ((TSS - RSS) / p) / (RSS / (n - p - 1))
p_val = pf(F, k, n-p, lower.tail=FALSE) 
print(p_val)
```

## d)

#### i)

Confidence interval is:

```{r}
new_obs_df = data.frame(crim=10, age=90, rm=5)
print(predict(lm_4, new_obs_df, interval="confidence", type="respons"))
```

#### ii)

```{r}
print(predict(lm_4, new_obs_df, interval="prediction", type="respons"))
```

#### iii)

Confidence interval is the interval for where we can say with a certain amount of level of probability that the our linear regression line is (so a confidence interval for our $\beta_{i}$ estimator).

The prediction interval is the interval where we can say a new observation (so a new y observation) must be within. This is will then contain the uncertainty from our $\beta_{i}$ and our irreducible errors.

The prediction interval must always be bigger then the confidence interval as it contains more uncertainty

#### iiii)

```{r}
#Autoplot will make all the relevant plots for us
autoplot(lm_4)
```

The QQ plot describes the distribution of our residuals to a normal distribution. If our residuals are normally distributed the plot will be a straight line (as both the residuals and the normal distribution compared to are equal). We see that our Q-Q plot our residuals are approximately normally distributed close to the 0 quantile, while it increases as we move away from it. This implies that the residuals are not normally distributed (maybe t-distributed based on the Q-Q plot) as we assume when creating the linear regression estimator.

The leverage is how large effect a sample from our data set has on the linear regression estimator, the main factor here is distance from the midpoint of the linear regressor in the feature space. A sample from our data set with a large residual, but low leverage will have a small effect on the linear regression model. The flagged samples in the plot are likely outliers

The scale-location plot is used to check the assumption of equal variance between all the residuals. We then plot each standardized residual to the prediction value $\hat{y}$. Equal variances implies no trend. Based on this it seems like the assumption of equal variance in each residual is not correct in our data.

Tukey-Anscombe plot is similare to scale-location just using the residual instead of squared standardized residuals. It should be centered in 0 ( 0 mean) and not have a trend.

## e)

####i) This is incorrect as $x_\text{male}$ and $x_\text{female}$ is encoding the same information.

####ii) we must remove one of the predictors. Which one it is has no meaning $y = \beta_0 + \beta_1*x_\text{male}$ would be a valid formulation.

#### iii)

if we pick bachelor as our reference category and chose to not include it, we would get: $y = \beta_0 + \beta_1x_\text{master} + \beta_2x_\text{phd}$

## f)

i)  False
ii) False (spÃ¸r studass om hva som er variance og bias)
iii) True
iv) False

# Problem 3

# Problem 4
